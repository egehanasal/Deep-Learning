{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = keras.datasets.imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = data.load_data(num_words=90000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data[0] is a list of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = data.get_word_index() #Retrieves a dict mapping words to their index in the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {k:(v+3) for k, v in word_index.items()}\n",
    "#dictionary.items() returns a view object that displays a list of a given dictionary's (key, value) tuple pair.\n",
    "#by making it a list or dictionary, tuples inside that object is reachable.\n",
    "#we are adding 3 to the each value for each key\n",
    "word_index[\"<PAD>\"] = 0 #will be used to make reviews the same length by adding 0 to the shorter ones\n",
    "word_index[\"<START>\"] = 1 #each texts first element is 1, so \"<START>\" will be printed before anything else\n",
    "word_index[\"<UNK>\"] = 2 #2 represents the unkown words in the data, \"<UNK>\" will be printed when an unknown word comes\n",
    "word_index[\"<UNUSED>\"] = 3 #3 represents the unkown words in the data, \"<UNUSED>\" will be printed when an unknown word comes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reversing the dict to get a dict that is mapping the index' to their key\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoding the data to readable words\n",
    "#gets the word that corresponds to that index and puts space after every word\n",
    "#if that index doesn't correspond to a word, puts a \"?\"\n",
    "def decode_review(text):\n",
    "    return \" \".join([reverse_word_index.get(i, \"?\") for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> please give this one a miss br br kristy swanson and the rest of the cast rendered terrible performances the show is flat flat flat br br i don't know how michael madison could have allowed this one on his plate he almost seemed to know this wasn't going to work out and his performance was quite lacklustre so all you madison fans give this a miss\n"
     ]
    }
   ],
   "source": [
    "print(decode_review(test_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68, 260)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data[0]), len(test_data[1]) #two different lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have to know the max length of the texts to determine the number of neurons or we can set a number and rearrange the train_data and test_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218, 189, 68, 260)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[0]), len(train_data[1]), len(test_data[0]), len(test_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### By setting maxLen=250, texts that are longer than 250 in train_data and test_data will be deleted and by setting value=word_index[\"<PAD\">\"], we are adding 0's to the texts that has shorter length than 250 and by setting padding=\"post\", we are adding the 0's to the \"end\" of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data, value=word_index[\"<PAD>\"], padding=\"post\", maxlen=250)\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data, value=word_index[\"<PAD>\"], padding=\"post\", maxlen=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 250, 250, 250)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[0]), len(train_data[1]), len(test_data[0]), len(test_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Embedding(90000, 16)) #16 dimensions\n",
    "#created 10000 word vectors. If two words have similar meanings, those vectors are\n",
    "#close to each other so the degree between them is very small.\n",
    "model.add(keras.layers.GlobalAveragePooling1D()) #puts the daha in a lower dimension\n",
    "model.add(keras.layers.Dense(16, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          1440000   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,440,289\n",
      "Trainable params: 1,440,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = train_data[:10000]\n",
    "x_train = train_data[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = train_labels[:10000]\n",
    "y_train = train_labels[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "30/30 [==============================] - 3s 57ms/step - loss: 0.6921 - accuracy: 0.5223 - val_loss: 0.6901 - val_accuracy: 0.4964\n",
      "Epoch 2/40\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.6856 - accuracy: 0.5473 - val_loss: 0.6815 - val_accuracy: 0.6965\n",
      "Epoch 3/40\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.6725 - accuracy: 0.6974 - val_loss: 0.6663 - val_accuracy: 0.7262\n",
      "Epoch 4/40\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.6504 - accuracy: 0.7515 - val_loss: 0.6425 - val_accuracy: 0.7621\n",
      "Epoch 5/40\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.6183 - accuracy: 0.7961 - val_loss: 0.6106 - val_accuracy: 0.7794\n",
      "Epoch 6/40\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.5775 - accuracy: 0.8177 - val_loss: 0.5722 - val_accuracy: 0.8046\n",
      "Epoch 7/40\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.5293 - accuracy: 0.8443 - val_loss: 0.5294 - val_accuracy: 0.8230\n",
      "Epoch 8/40\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.4788 - accuracy: 0.8641 - val_loss: 0.4874 - val_accuracy: 0.8371\n",
      "Epoch 9/40\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.4306 - accuracy: 0.8801 - val_loss: 0.4514 - val_accuracy: 0.8424\n",
      "Epoch 10/40\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.3863 - accuracy: 0.8933 - val_loss: 0.4172 - val_accuracy: 0.8547\n",
      "Epoch 11/40\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.3474 - accuracy: 0.9025 - val_loss: 0.3899 - val_accuracy: 0.8619\n",
      "Epoch 12/40\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.3141 - accuracy: 0.9105 - val_loss: 0.3670 - val_accuracy: 0.8689\n",
      "Epoch 13/40\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.2847 - accuracy: 0.9186 - val_loss: 0.3491 - val_accuracy: 0.8725\n",
      "Epoch 14/40\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.2601 - accuracy: 0.9255 - val_loss: 0.3347 - val_accuracy: 0.8763\n",
      "Epoch 15/40\n",
      "30/30 [==============================] - 1s 50ms/step - loss: 0.2376 - accuracy: 0.9324 - val_loss: 0.3229 - val_accuracy: 0.8777\n",
      "Epoch 16/40\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.2183 - accuracy: 0.9371 - val_loss: 0.3131 - val_accuracy: 0.8799\n",
      "Epoch 17/40\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.2007 - accuracy: 0.9449 - val_loss: 0.3053 - val_accuracy: 0.8827\n",
      "Epoch 18/40\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.1851 - accuracy: 0.9495 - val_loss: 0.2983 - val_accuracy: 0.8839\n",
      "Epoch 19/40\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.1710 - accuracy: 0.9542 - val_loss: 0.2929 - val_accuracy: 0.8866\n",
      "Epoch 20/40\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.1584 - accuracy: 0.9596 - val_loss: 0.2884 - val_accuracy: 0.8855\n",
      "Epoch 21/40\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.1466 - accuracy: 0.9627 - val_loss: 0.2850 - val_accuracy: 0.8873\n",
      "Epoch 22/40\n",
      "30/30 [==============================] - 1s 47ms/step - loss: 0.1359 - accuracy: 0.9668 - val_loss: 0.2819 - val_accuracy: 0.8882\n",
      "Epoch 23/40\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.1261 - accuracy: 0.9700 - val_loss: 0.2805 - val_accuracy: 0.8877\n",
      "Epoch 24/40\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.1171 - accuracy: 0.9737 - val_loss: 0.2782 - val_accuracy: 0.8881\n",
      "Epoch 25/40\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.1092 - accuracy: 0.9755 - val_loss: 0.2789 - val_accuracy: 0.8894\n",
      "Epoch 26/40\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.1018 - accuracy: 0.9783 - val_loss: 0.2776 - val_accuracy: 0.8885\n",
      "Epoch 27/40\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.0944 - accuracy: 0.9807 - val_loss: 0.2774 - val_accuracy: 0.8900\n",
      "Epoch 28/40\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.0883 - accuracy: 0.9827 - val_loss: 0.2768 - val_accuracy: 0.8899\n",
      "Epoch 29/40\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.0826 - accuracy: 0.9835 - val_loss: 0.2770 - val_accuracy: 0.8912\n",
      "Epoch 30/40\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.0764 - accuracy: 0.9859 - val_loss: 0.2777 - val_accuracy: 0.8914\n",
      "Epoch 31/40\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.0714 - accuracy: 0.9876 - val_loss: 0.2797 - val_accuracy: 0.8898\n",
      "Epoch 32/40\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.0668 - accuracy: 0.9884 - val_loss: 0.2805 - val_accuracy: 0.8901\n",
      "Epoch 33/40\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.0628 - accuracy: 0.9896 - val_loss: 0.2816 - val_accuracy: 0.8897\n",
      "Epoch 34/40\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.0585 - accuracy: 0.9904 - val_loss: 0.2844 - val_accuracy: 0.8886\n",
      "Epoch 35/40\n",
      "30/30 [==============================] - 1s 47ms/step - loss: 0.0552 - accuracy: 0.9908 - val_loss: 0.2849 - val_accuracy: 0.8888\n",
      "Epoch 36/40\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.0515 - accuracy: 0.9922 - val_loss: 0.2872 - val_accuracy: 0.8888\n",
      "Epoch 37/40\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.0485 - accuracy: 0.9931 - val_loss: 0.2894 - val_accuracy: 0.8894\n",
      "Epoch 38/40\n",
      "30/30 [==============================] - 1s 46ms/step - loss: 0.0455 - accuracy: 0.9934 - val_loss: 0.2908 - val_accuracy: 0.8882\n",
      "Epoch 39/40\n",
      "30/30 [==============================] - 1s 47ms/step - loss: 0.0429 - accuracy: 0.9943 - val_loss: 0.2934 - val_accuracy: 0.8890\n",
      "Epoch 40/40\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.0406 - accuracy: 0.9949 - val_loss: 0.2956 - val_accuracy: 0.8883\n"
     ]
    }
   ],
   "source": [
    "fitModel = model.fit(x_train, y_train, epochs=40, batch_size=512, validation_data=(x_val, y_val), verbose=1)\n",
    "#batch_size: number of the movie reviews that we are giving every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 1s 2ms/step - loss: 0.3306 - accuracy: 0.8717\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.h5\") #to load this saved model: model = keras.models.load_model(\"model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
