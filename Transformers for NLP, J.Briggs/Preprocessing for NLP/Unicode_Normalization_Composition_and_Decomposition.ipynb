{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unicode normalization is used to normalize different but similiar characters.  Two different types of equivalence here, canonical and compatibility equivalence.\n",
        "\n",
        "* **Canonical equivalence** means both forms are fundamentally the same and when rendered are indistinguishable. For example we can take the unicode for `'Ç' \\u00C7` or the unicode for `'C' \\u0043` and `'̧' \\u0327`, when the latter two characters are rendered together they look the same as the first character:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ç Ç\n"
          ]
        }
      ],
      "source": [
        "print(\"\\u00C7\", \"\\u0043\"+\"\\u0327\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, if we print these characters seperately, we can see very clearly that they are not the same"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ç C ̧\n"
          ]
        }
      ],
      "source": [
        "print(\"\\u00C7\", \"\\u0043\", \"\\u0327\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These are examples of canonical equivalence, but we also have compatibility equivalence.\n",
        "\n",
        "**Compatibility equivalence** refers to the formatting differences between characters, which includes (but is not limited to):\n",
        "\n",
        "* font variants\n",
        "* cursive forms\n",
        "* circled characters\n",
        "* width variation\n",
        "* size changes\n",
        "* rotation\n",
        "* superscript and subscript\n",
        "* fractions\n",
        "\n",
        "In this case we can see a difference between the rendered characters, for example between `ℌ` and `H`, or `½` and `1 ⁄ 2`.\n",
        "\n",
        "For many of these examples which are either canonical equivalents (Ç ↔ C ̧ ) or compatibility equivalents (½ → 1 ⁄ 2), if we compare if these different forms are equal, we will find that they are not:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"Ç\" == \"Ç\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"ℌ\" == \"H\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So it is in these cases that we use unicode normalization to normalize our characters into matching pairs. As there are different forms of equivalence, there are also different forms of normalization. These are all called **N**ormal **F**orm, and there are four different methods:\n",
        "\n",
        "* NFD: Canonical decomposition `Ç → C`\n",
        "* NFC:Canoncial decomposition followed by canonical composition\t`Ç → C ̧ → Ç`\n",
        "* NFKD:\tCompatibility decomposition\t`ℌ ̧ → H` ̧\n",
        "* NFKC:\tCompatibility decomposition followed by canonical composition `ℌ ̧ → H ̧ → Ḩ`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "y-n_0oBDENKg"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Ç'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "c_with_cedilla = \"\\u00C7\"  # Latin capital C with cedilla (single character)\n",
        "c_with_cedilla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Ç'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "c_plus_cedilla = \"\\u0043\\u0327\"  # \\u0043 = Latin capital C, \\u0327 = 'combining cedilla' (two characters)\n",
        "c_plus_cedilla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "c_with_cedilla == c_plus_cedilla"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we perform NFD on our C with cedilla character \\u00C7, we decompose the character into it's smaller components, which are the Latin capital C character, and combining cedilla character \\u0043 + \\u0327. This means that if we compare an NFD normalized C with cedilla character to both the C character and the cedilla character, we will return true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import unicodedata\n",
        "unicodedata.normalize('NFD', c_with_cedilla) == c_plus_cedilla"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, if we perform NFC on our C with cedilla character \\u00C7, we decompose the character into the smaller components \\u0043 + \\u0327, and then compose them back to \\u00C7, and so they will not match"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unicodedata.normalize('NFC', c_with_cedilla) == c_plus_cedilla"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "But if we switch the NFC encoding to instead be performed on our two characters \\u0043 + \\u0327, they will first be decomposed (which will do nothing as they are already decomposed), then compose them into the single \\u00C7 characte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "c_with_cedilla == unicodedata.normalize('NFC', c_plus_cedilla)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The NFK encodings do not decompose characters into smaller components, they decompose characters into their normal versions. For example if we take the fancy format ℌ \\u210B, we cannot decompose this into multiple characters and so NFD or NFC encoding will do nothing. However, if we apply NFKD, we will find that our fancy ℌ \\u210B becomes a plain, boring H \\u0048"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'H'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unicodedata.normalize('NFKD', 'ℌ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "But, what if we merge our fancy ℌ \\u210B with a combining cedilla \\u0328 character?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'ℋ̧'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\\u210B\\u0327\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Applying our compatibility decomposition normalization (NFKD) gives us a capital H character, and a combining cedilla character as two seperate encodings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "b'H\\xcc\\xa7'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unicodedata.normalize('NFKD', \"\\u210B\\u0327\").encode('utf-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "But if we apply NFKC, we first perform compatibility decomposition, into the two seperate characters, before merging them during canonical composition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "b'\\xe1\\xb8\\xa8'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unicodedata.normalize('NFKC', \"\\u210B\\u0327\").encode('utf-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because the only difference between these two methods is a canonical composition, we see no difference between the two character sets when they are rendered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('Ḩ', 'Ḩ')"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unicodedata.normalize('NFKC', \"\\u210B\\u0327\"), unicodedata.normalize('NFKD', \"\\u210B\\u0327\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Unicode Normalization - Composition and Decomposition.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
