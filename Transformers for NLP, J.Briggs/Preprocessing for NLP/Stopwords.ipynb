{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stopwords.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRRungVhUx5X"
      },
      "source": [
        "tweet = \"\"\"I’m amazed how often in practice, not only does a @huggingface NLP model solve your problem, but one of their public finetuned checkpoints, is good enough for the job.\n",
        "\n",
        "Both impressed, and a little disappointed how rarely I get to actually train a model that matters :(\"\"\""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "bYB0CdzONc7x",
        "outputId": "86cc7ed9-16aa-4e5e-b2c9-8994584f678c"
      },
      "source": [
        "tweet"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I’m amazed how often in practice, not only does a @huggingface NLP model solve your problem, but one of their public finetuned checkpoints, is good enough for the job.\\n\\nBoth impressed, and a little disappointed how rarely I get to actually train a model that matters :('"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNrK8ghcUy0N"
      },
      "source": [
        "* We will be using the NLTK library for removing stopwords. NLTK comes with several stopword corpora, we will be using the English corpus. This corpus contains a huge number of English stopwords like a, the, be, for, do, and so on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgHtng57S2DE",
        "outputId": "54c35bfe-629e-4909-9f9b-aadb7a767739"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcIdVSA6Nc5R"
      },
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Pfj3OiMNc2h",
        "outputId": "309aef65-d8d6-497d-de78-5f8ec8c63f36"
      },
      "source": [
        "stop_words = stopwords.words(\"english\")\n",
        "stop_words[:10]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr6ZO13fU8sB"
      },
      "source": [
        "* To optimize the speed of the stopword lookup we can convert stop_words to a set object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmVUIyR7Nczw"
      },
      "source": [
        "stop_words = set(stop_words)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcBDY2zMVI0J"
      },
      "source": [
        "First we need to lowercase our text (because all of our stopwords are lowercased)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onVA3xZsNcxA",
        "outputId": "20a51b08-d96d-4f73-ca50-91133d39751d"
      },
      "source": [
        "tweet = tweet.lower().split()\n",
        "tweet"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i’m',\n",
              " 'amazed',\n",
              " 'how',\n",
              " 'often',\n",
              " 'in',\n",
              " 'practice,',\n",
              " 'not',\n",
              " 'only',\n",
              " 'does',\n",
              " 'a',\n",
              " '@huggingface',\n",
              " 'nlp',\n",
              " 'model',\n",
              " 'solve',\n",
              " 'your',\n",
              " 'problem,',\n",
              " 'but',\n",
              " 'one',\n",
              " 'of',\n",
              " 'their',\n",
              " 'public',\n",
              " 'finetuned',\n",
              " 'checkpoints,',\n",
              " 'is',\n",
              " 'good',\n",
              " 'enough',\n",
              " 'for',\n",
              " 'the',\n",
              " 'job.',\n",
              " 'both',\n",
              " 'impressed,',\n",
              " 'and',\n",
              " 'a',\n",
              " 'little',\n",
              " 'disappointed',\n",
              " 'how',\n",
              " 'rarely',\n",
              " 'i',\n",
              " 'get',\n",
              " 'to',\n",
              " 'actually',\n",
              " 'train',\n",
              " 'a',\n",
              " 'model',\n",
              " 'that',\n",
              " 'matters',\n",
              " ':(']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3c7BUY9TEfA"
      },
      "source": [
        "tweet_no_stopwords = [word for word in tweet if word not in stop_words]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAthyTigTEcS",
        "outputId": "584e4ae6-f4fb-4560-a7c2-2a35ad960f29"
      },
      "source": [
        "print(\"With stopwords: \", \" \".join(tweet))\n",
        "print(\"\\nWithout: \", \" \".join(tweet_no_stopwords))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "With stopwords:  i’m amazed how often in practice, not only does a @huggingface nlp model solve your problem, but one of their public finetuned checkpoints, is good enough for the job. both impressed, and a little disappointed how rarely i get to actually train a model that matters :(\n",
            "\n",
            "Without:  i’m amazed often practice, @huggingface nlp model solve problem, one public finetuned checkpoints, good enough job. impressed, little disappointed rarely get actually train model matters :(\n"
          ]
        }
      ]
    }
  ]
}