{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transfer_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33pAPKoD3G9B"
      },
      "source": [
        "# Part1: Feature Extraction\n",
        "\n",
        "To improve our model(s), we could spend a while trying different configurations, adding more layers, changing the learning rate, adjusting the number of neurons per layer and more.\n",
        "\n",
        "However, doing this is very time consuming.\n",
        "\n",
        "Luckily, there's a technique we can use to save time.\n",
        "\n",
        "It's called **transfer learning**, in other words, taking the patterns (also called weights) another model has learned from another problem and using them for our own problem.\n",
        "\n",
        "There are 2 main benefits:\n",
        "1. Can leverage an existing neural network architecture proven to work on problems similar to our own.\n",
        "2. Can leverage a working neural network architecture which has already learned patterns on similar data to our own, then we can adapt those patterns to our own data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m2vDF8JOMNQ"
      },
      "source": [
        "What this means is, instead of hand-crafting our own neural network architectures or building them from scratch, we can utilise models which have worked for others.\n",
        "\n",
        "And instead of training our own models from scratch on our own datasets, we can take the patterns a model has learned from datasets such as [ImageNet](http://www.image-net.org/) (millions of images of different objects) and use them as the foundation of our own. Doing this often leads to getting great results with less data.\n",
        "\n",
        "**Transfer learning often allows you to get great results with less data.**\n",
        "\n",
        "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/04-transfer-learning-feature-extraction.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhx4o0_HF8Li"
      },
      "source": [
        "## Download and become one with the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmfwX4FkF8I1",
        "outputId": "8c1b84e6-7e22-47cd-c53e-a77375576bed"
      },
      "source": [
        "# Get data (10% of 10 food classes from Food101)\n",
        "import zipfile\n",
        "\n",
        "# Download the data\n",
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
        "\n",
        "# Unzip the downloaded file\n",
        "zip_ref = zipfile.ZipFile(\"10_food_classes_10_percent.zip\")\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-22 20:33:32--  https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 209.85.147.128, 142.250.125.128, 142.250.136.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|209.85.147.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 168546183 (161M) [application/zip]\n",
            "Saving to: ‘10_food_classes_10_percent.zip’\n",
            "\n",
            "10_food_classes_10_ 100%[===================>] 160.74M  92.4MB/s    in 1.7s    \n",
            "\n",
            "2021-09-22 20:33:34 (92.4 MB/s) - ‘10_food_classes_10_percent.zip’ saved [168546183/168546183]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IvFB83VF8GO",
        "outputId": "1675b71a-1ac4-4168-ccea-363804e89be5"
      },
      "source": [
        "# How many images in each folder\n",
        "import os\n",
        "\n",
        "# Walk through 10 percent data directory\n",
        "for dirpath, dirnames, filenames in os.walk(\"10_food_classes_10_percent\"):\n",
        "  print(\"There are \" + str(len(dirnames)) + \" directories and \" + str(len(filenames)) + \" images in \" + dirpath)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 directories and 0 images in 10_food_classes_10_percent\n",
            "There are 10 directories and 0 images in 10_food_classes_10_percent/train\n",
            "There are 0 directories and 75 images in 10_food_classes_10_percent/train/ramen\n",
            "There are 0 directories and 75 images in 10_food_classes_10_percent/train/chicken_curry\n",
            "There are 0 directories and 75 images in 10_food_classes_10_percent/train/chicken_wings\n",
            "There are 0 directories and 75 images in 10_food_classes_10_percent/train/ice_cream\n",
            "There are 0 directories and 75 images in 10_food_classes_10_percent/train/grilled_salmon\n",
            "There are 0 directories and 75 images in 10_food_classes_10_percent/train/steak\n",
            "There are 0 directories and 75 images in 10_food_classes_10_percent/train/pizza\n",
            "There are 0 directories and 75 images in 10_food_classes_10_percent/train/sushi\n",
            "There are 0 directories and 75 images in 10_food_classes_10_percent/train/fried_rice\n",
            "There are 0 directories and 75 images in 10_food_classes_10_percent/train/hamburger\n",
            "There are 10 directories and 0 images in 10_food_classes_10_percent/test\n",
            "There are 0 directories and 250 images in 10_food_classes_10_percent/test/ramen\n",
            "There are 0 directories and 250 images in 10_food_classes_10_percent/test/chicken_curry\n",
            "There are 0 directories and 250 images in 10_food_classes_10_percent/test/chicken_wings\n",
            "There are 0 directories and 250 images in 10_food_classes_10_percent/test/ice_cream\n",
            "There are 0 directories and 250 images in 10_food_classes_10_percent/test/grilled_salmon\n",
            "There are 0 directories and 250 images in 10_food_classes_10_percent/test/steak\n",
            "There are 0 directories and 250 images in 10_food_classes_10_percent/test/pizza\n",
            "There are 0 directories and 250 images in 10_food_classes_10_percent/test/sushi\n",
            "There are 0 directories and 250 images in 10_food_classes_10_percent/test/fried_rice\n",
            "There are 0 directories and 250 images in 10_food_classes_10_percent/test/hamburger\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZEN0y5oF8Dn"
      },
      "source": [
        "## Creating data loaders (preparing the data)\n",
        "\n",
        "We'll use the `ImageDataGenerator`class to load in our images in batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9xzlaO7lL9r",
        "outputId": "2d0bf17d-9f2d-4f85-9129-0264da903c0f"
      },
      "source": [
        "# Setup the inputs\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "IMAGE_SHAPE = (224,224)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 5\n",
        "\n",
        "train_dir = \"10_food_classes_10_percent/train/\"\n",
        "test_dir = \"10_food_classes_10_percent/test\"\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1/255.0,)\n",
        "test_datagen = ImageDataGenerator(rescale=1/255.0)\n",
        "\n",
        "print(\"Training images:\")\n",
        "train_data_10_percent = train_datagen.flow_from_directory(train_dir,\n",
        "                                                          target_size=IMAGE_SHAPE,\n",
        "                                                          batch_size=BATCH_SIZE,\n",
        "                                                          class_mode=\"categorical\")\n",
        "print(\"Testing images:\")\n",
        "test_data = test_datagen.flow_from_directory(test_dir,\n",
        "                                                        target_size=IMAGE_SHAPE,\n",
        "                                                        batch_size=BATCH_SIZE,\n",
        "                                                        class_mode=\"categorical\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training images:\n",
            "Found 750 images belonging to 10 classes.\n",
            "Testing images:\n",
            "Found 2500 images belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qAbBJn1lMBb"
      },
      "source": [
        "## Setting up callbacks (things to run whilst our model trains)\n",
        "\n",
        "Callbacks are extra functionality you can add to your models to be performed during or after training. Some of the most popular callbacks:\n",
        "\n",
        "- **Experiment tracking with TensorBoard** - log the performance of multiple models and then view and compare these models in a visual way on TensorBoard (a dashboard for inspecting neural network parameters). Helpful to compare the results of different models on your data.\n",
        "- **Model checkpointing** - save your model as it trains so you can stop training if needed and come back to continue off where you left. Helpful if training takes a long time and can't be done in one sitting.\n",
        "- **Early stopping** - leave your model training for an arbitrary amount of time and have it stop training automatically when it ceases to improve. Helpful when you've got a large dataset and don't know how long training will take."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ3qdcqzcF0B"
      },
      "source": [
        "**The TensorBoard callback**'s main functionality is saving a model's training performance metrics to a specified `log_dir`.\n",
        "\n",
        "By default, logs are recorded every epoch using the `update_freq='epoch'` parameter. This is a good default since tracking model performance too often can slow down model training.\n",
        "\n",
        "To track our modelling experiments using TensorBoard, let's create a function which creates a TensorBoard callback for us."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlvxfXowcSaK"
      },
      "source": [
        "> 🔑 **Note:** We create a function for creating a TensorBoard callback because as we'll see later on, each model needs its own TensorBoard callback instance (so the function will create a new one each time it's run)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOF_X2Q5lMEa"
      },
      "source": [
        "# Create TensorBoard callback (functionized because we need to create a new one for each model)\n",
        "import datetime\n",
        "\n",
        "def create_tensorboard_callback(dir_name, experiment_name):\n",
        "  log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
        "  print(\"Saving TensorBoard log files to: \" + log_dir)\n",
        "  return tensorboard_callback"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ4y1aGsAp00"
      },
      "source": [
        "🔑 **Note:** You can customize the directory where your TensorBoard logs (model training metrics) get saved to whatever you like. The `log_dir` parameter we've created above is only one option"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5JQdI39lMHq"
      },
      "source": [
        "## Creating models using TensorFlow Hub\n",
        "\n",
        "In the past, we've used TensorFlow to create our own models layer by layer from scratch\n",
        "\n",
        "Now we're going to do a similar process, except the majority of our model's layers are going to come from TensorFlow Hub.\n",
        "\n",
        "We can access pretrained models on: https://tfhub.dev/\n",
        "\n",
        "Browsing the TensorFlow Hub page and sorting for image classification, we found the following feature vector model link: https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\n",
        "\n",
        "We're going to use two models from TensorFlow Hub:\n",
        "1. [ResNetV2](https://arxiv.org/abs/1603.05027) -  a state of the art computer vision model architecture from 2016.\n",
        "2. [EfficientNet](https://arxiv.org/abs/1905.11946) - a state of the art computer vision architecture from 2019.\n",
        "\n",
        "State of the art means that at some point, both of these models have achieved the lowest error rate on [ImageNet (ILSVRC-2012-CLS)](http://www.image-net.org/), the gold standard of computer vision benchmarks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kM-GoOleZUz"
      },
      "source": [
        "1. **transfer learning** is when you take a pretrained model as it is and apply it to your task without any changes. \n",
        "\n",
        "  * For example, many computer vision models are pretrained on the ImageNet dataset which contains 1000 different classes of images. This means passing a single image to this model will produce 1000 different prediction probability values (1 for each class). \n",
        "\n",
        "    * This is helpful if you have 1000 classes of image you'd like to classify and they're all the same as the ImageNet classes, however, it's not helpful if you want to classify only a small subset of classes (such as 10 different kinds of food). Model's with `\"/classification\"` in their name on TensorFlow Hub provide this kind of functionality.\n",
        "\n",
        "2. **Feature extraction transfer learning** is when you take the underlying patterns (also called weights) a pretrained model has learned and adjust its outputs to be more suited to your problem. \n",
        "\n",
        "  * For example, say the pretrained model you were using had 236 different layers (EfficientNetB0 has 236 layers), but the top layer outputs 1000 classes because it was pretrained on ImageNet. To adjust this to your own problem, you might remove the original activation layer and replace it with your own but with the right number of output classes. The important part here is that **only the top few layers become trainable, the rest remain frozen**. \n",
        "\n",
        "    * This way all the underlying patterns remain in the rest of the layers and you can utilise them for your own problem. This kind of transfer learning is very helpful when your data is similar to the data a model has been pretrained on.\n",
        "\n",
        "3. **Fine-tuning transfer learning** is when you take the underlying patterns (also called weights) of a pretrained model and adjust (fine-tune) them to your own problem. \n",
        "\n",
        "    * This usually means training **some, many or all** of the layers in the pretrained model. This is useful when you've got a large dataset (e.g. 100+ images per class) where your data is slightly different to the data the original model was trained on.\n",
        "\n",
        "A common workflow is to \"freeze\" all of the learned patterns in the bottom layers of a pretrained model so they're untrainable. And then train the top 2-3 layers of so the pretrained model can adjust its outputs to your custom data (**feature extraction**).\n",
        "\n",
        "After you've trained the top 2-3 layers, you can then gradually \"unfreeze\" more and more layers and run the training process on your own data to further **fine-tune** the pretrained model.\n",
        "\n",
        "The lower a layer is in a computer vision model as in, the closer it is to the input layer, the larger the features it learn. For example, a bottom layer in a computer vision model to identify images of cats or dogs might learn the outline of legs, where as, layers closer to the output might learn the shape of teeth. Often, you'll want the larger features (learned patterns are also called features) to remain, since these are similar for both animals, where as, the differences remain in the more fine-grained features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQSg06xilMKx"
      },
      "source": [
        "# Let's compare the following 2 models\n",
        "resnet_url = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4\"\n",
        "efficientnet_url = \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QztbtldjlMN3"
      },
      "source": [
        "# Import dependencies\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cw-13Xl5UDCq",
        "outputId": "d44549e9-8e15-44eb-8fcb-ee90a033a1c1"
      },
      "source": [
        "IMAGE_SHAPE + (3,)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(224, 224, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJ6yem3fS0T6"
      },
      "source": [
        "# Let's make a create_model() function to create a model from a URL\n",
        "def create_model(model_url, num_classes=10):\n",
        "  \"\"\"\n",
        "  Takes a TensorFlow Hub URL and creates a Keras Sequential model with it.\n",
        "  \n",
        "  Args:\n",
        "    model_url (str): A TensorFlow Hub feature extraction URL.\n",
        "    num_classes (int): Number of output neurons in the output layer,\n",
        "      should be equal to number of target classes, default 10.\n",
        "\n",
        "  Returns:\n",
        "    An uncompiled Keras Sequential model with model_url as feature extractor \n",
        "    layer and Dense output layer with num_classes output neurons.\n",
        "  \"\"\"\n",
        "  # Download the pretrained model and save it as a Keras layer\n",
        "  feature_extractor_layer = hub.KerasLayer(model_url,\n",
        "                                           trainable=False, # freeze the already learned pattern\n",
        "                                           name=\"feature_extraction_layer\",\n",
        "                                           input_shape=IMAGE_SHAPE + (3,)) \n",
        "  \n",
        "  # Create our own model\n",
        "  model = tf.keras.Sequential([\n",
        "    feature_extractor_layer,\n",
        "    layers.Dense(num_classes, activation=\"softmax\", name=\"output_layer\")\n",
        "  ])\n",
        "\n",
        "  return model"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnSXNW47Uf2h"
      },
      "source": [
        "### Creating and testing ResNet TensorFlow Hub Feature Extraction model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ro-4mFYWS0Xk"
      },
      "source": [
        "# Create Resnet model\n",
        "resnet_model = create_model(resnet_url,\n",
        "                            num_classes=train_data_10_percent.num_classes)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iAXIs_vS0a4",
        "outputId": "eb294bc7-6e3f-4620-bc59-85dce58484da"
      },
      "source": [
        "resnet_model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "feature_extraction_layer (Ke (None, 2048)              23564800  \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 23,585,290\n",
            "Trainable params: 20,490\n",
            "Non-trainable params: 23,564,800\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65WqBBL3WXy9"
      },
      "source": [
        "Those non-trainable params are "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFcLcfEPS0d8"
      },
      "source": [
        "# Compile our resnet model\n",
        "resnet_model.compile(loss=\"categorical_crossentropy\",\n",
        "                     optimizer=\"Adam\",\n",
        "                     metrics=[\"accuracy\"])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pumI68OS0g4",
        "outputId": "d296d804-d2a2-4467-848d-56a653167d37"
      },
      "source": [
        "# Let's fit our ResNet model to the data (10 percent of 10 classes)\n",
        "resnet_history = resnet_model.fit(train_data_10_percent,\n",
        "                 epochs=5,\n",
        "                 steps_per_epoch=len(train_data_10_percent),\n",
        "                 validation_data=test_data,\n",
        "                 validation_steps=len(test_data),\n",
        "                 callbacks=[create_tensorboard_callback(dir_name=\"tensorflow_hub\",\n",
        "                                                        experiment_name=\"resnet50V2\")])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TensorBoard log files to: tensorflow_hub/resnet50V2/20210922-211251\n",
            "Epoch 1/5\n",
            "24/24 [==============================] - 61s 1s/step - loss: 2.0147 - accuracy: 0.3240 - val_loss: 1.2744 - val_accuracy: 0.5920\n",
            "Epoch 2/5\n",
            "24/24 [==============================] - 22s 940ms/step - loss: 0.9592 - accuracy: 0.7107 - val_loss: 0.8733 - val_accuracy: 0.7096\n",
            "Epoch 3/5\n",
            "24/24 [==============================] - 22s 943ms/step - loss: 0.6487 - accuracy: 0.8200 - val_loss: 0.7742 - val_accuracy: 0.7484\n",
            "Epoch 4/5\n",
            "24/24 [==============================] - 22s 937ms/step - loss: 0.5091 - accuracy: 0.8653 - val_loss: 0.7075 - val_accuracy: 0.7740\n",
            "Epoch 5/5\n",
            "24/24 [==============================] - 22s 943ms/step - loss: 0.4069 - accuracy: 0.8933 - val_loss: 0.6654 - val_accuracy: 0.7864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUfDte3pZN6P"
      },
      "source": [
        "Our transfer learning feature extractor model out performed ALL of the prevous models we built by hand... and in a quicker training time AND with only 10% of the training images "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rzJxvFkWAcL"
      },
      "source": [
        "# Let's create a function to plot our loss curves"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}